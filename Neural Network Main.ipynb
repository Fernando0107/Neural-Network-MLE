{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "## Juan Fernando Gonzalez\n",
    "20170085"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import scipy.optimize as op\n",
    "from utils import mnist_reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns the sigmoid activation of a given input.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    s = (1/(1 + np.e**(-x)))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(thetas, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the all activations of all neurons of a layer.\n",
    "    \n",
    "    activations:\n",
    "        array: Keep the activation given the sigmoid function. (vector)\n",
    "    Bias:\n",
    "        Bias added to our activations. (vector)\n",
    "    thetas: \n",
    "        Weight of each connection. (Matrix)\n",
    "    \n",
    "    \"\"\"\n",
    "    print('\\n\\t -------------------------- Feed Forward ------------------------------ \\n')\n",
    "    activations = []\n",
    "    activations.append(X)                     # Input Neurons (layer 0)\n",
    "    bias = np.ones(len(X)).reshape(len(X), 1) # Bias added (vector of ones) \n",
    "    \n",
    "    for i in range (len(thetas)):\n",
    "        \n",
    "        print('A: ',activations[i], '\\n')            # Actual activation \n",
    "        print('Thetas: ',thetas[i], '\\n')             # Actual Theta\n",
    "        print('\\t\\t\\t\\tShape',thetas[i].shape, '\\n')        # Shape of the actual theta\n",
    "        \n",
    "                                   # np.dot(activations(with bias), current_theta(weigh))\n",
    "        activations.append(sigmoid(np.dot(np.hstack((bias, activations[i])), thetas[i].T))) \n",
    "        \n",
    "        print('Current Activation: \\n' , activations[i + 1], '\\n')\n",
    "        print('\\t --------------------------------------------------------------------- \\n')\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(act, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = (-1/m) * (np.dot(np.log(A), Y.T) + np.dot(log(1 - A), 1 - Y.T)) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(thetas, X, Y):\n",
    "    \n",
    "    m , neu = X.shape\n",
    "    \n",
    "    delta = [None] * len(thetas)\n",
    "    \n",
    "    activations = feed_forward(thetas, X)\n",
    "    \n",
    "    print('Activations: \\n' , activations)\n",
    "    \n",
    "    # Last layer error\n",
    "    delta_ = activations[-1] - Y\n",
    "    delta.append(delta_)\n",
    "    \n",
    "    print(\"\\nLast Layer Error: \\n \")\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in reversed(range(1, len(thetas) -1)):\n",
    "        \n",
    "        layers_error = np.dot(np.dot(thetas[i].T[1:,:], delta_), np.dot(activations[i], (1 - activations[i])))\n",
    "        \n",
    "        delta.append(layers_error)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return delta\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thetas calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brain_thatas(layer_dims):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a Theta-Matrix array.\n",
    "    \n",
    "    L:\n",
    "        Get layers lenght.\n",
    "    l:\n",
    "        Iterator; get the dim of the current theta.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    thetas = []\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        thetas.append(np.random.randn(layer_dims[l], layer_dims[l - 1] + 1) * 0.01)\n",
    "\n",
    "        \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data normalized shape (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "norm = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_scaled = norm.fit_transform(X_train)\n",
    "\n",
    "print('Training data normalized shape' , X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data normalized shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "X_test_scaled = norm.fit_transform(X_test)\n",
    "df_normalized_test = pd.DataFrame(X_test_scaled)\n",
    "\n",
    "print('Training data normalized shape' , X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Neurons on the Output Layer:  10\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(y_train)\n",
    "Y_test = pd.get_dummies(y_test)\n",
    "\n",
    "\n",
    "print('No. of Neurons on the Output Layer: ', Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenL_1 = np.zeros((3, 1))\n",
    "\n",
    "hiddenL_2 = np.zeros((2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Layers =  input layer + hidden layers + output layer\n",
    "\n",
    "total_l = []\n",
    "\n",
    "total_l.append(X_train_scaled.shape[1])\n",
    "total_l.append(len(hiddenL_1))\n",
    "total_l.append(len(hiddenL_2))\n",
    "total_l.append(Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:  [784, 3, 2, 10]\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions: ', total_l)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights (Thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t --------------- Thetas ---------------\n",
      "\n",
      "Theta (0): \n",
      " [[ 0.01624345 -0.00611756 -0.00528172 ... -0.00359224  0.00505382\n",
      "   0.01217941]\n",
      " [-0.01940681 -0.00806178  0.00049062 ...  0.00626906  0.00299825\n",
      "  -0.01856641]\n",
      " [-0.02151043  0.00136301  0.00683356 ... -0.00045206  0.00342846\n",
      "  -0.02164928]]\n",
      "\t\tShape:  (3, 785) \n",
      "\n",
      "Theta (1): \n",
      " [[ 0.0191183  -0.00303351 -0.01434214  0.0011153 ]\n",
      " [-0.01271168  0.00066393  0.00540602 -0.01318897]]\n",
      "\t\tShape:  (2, 4) \n",
      "\n",
      "Theta (2): \n",
      " [[ 0.00845426  0.00131092  0.00349084]\n",
      " [ 0.00404068  0.00512434  0.01120362]\n",
      " [ 0.00860454  0.0048691  -0.0076434 ]\n",
      " [ 0.00286331 -0.00557834 -0.01448752]\n",
      " [-0.00041377 -0.0091281   0.01135141]\n",
      " [-0.00037709  0.00045263  0.00775206]\n",
      " [-0.00418124 -0.00256595  0.01164671]\n",
      " [-0.01383679 -0.00866955 -0.00133694]\n",
      " [ 0.00784789  0.00249792 -0.01157517]\n",
      " [-0.00919774  0.0169446   0.00316269]]\n",
      "\t\tShape:  (10, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thetas = brain_thatas(total_l)\n",
    "\n",
    "print('\\t --------------- Thetas ---------------\\n')\n",
    "for i in range (len(thetas)):\n",
    "    print('Theta (' + str(i) +'): \\n', thetas[i])\n",
    "    print('\\t\\tShape: ',  thetas[i].shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Activations: \\n' ,feed_forward(thetas, X_train_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t -------------------------- Feed Forward ------------------------------ \n",
      "\n",
      "A:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "Thetas:  [[ 0.01624345 -0.00611756 -0.00528172 ... -0.00359224  0.00505382\n",
      "   0.01217941]\n",
      " [-0.01940681 -0.00806178  0.00049062 ...  0.00626906  0.00299825\n",
      "  -0.01856641]\n",
      " [-0.02151043  0.00136301  0.00683356 ... -0.00045206  0.00342846\n",
      "  -0.02164928]] \n",
      "\n",
      "\t\t\t\tShape (3, 785) \n",
      "\n",
      "Current Activation: \n",
      " [[0.52728472 0.59161295 0.51185904]\n",
      " [0.54576637 0.55399063 0.51523255]\n",
      " [0.51812192 0.52437305 0.49042856]\n",
      " ...\n",
      " [0.5171917  0.56048631 0.49979778]\n",
      " [0.51135008 0.5171699  0.49674573]\n",
      " [0.52124519 0.49500264 0.49222561]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "A:  [[0.52728472 0.59161295 0.51185904]\n",
      " [0.54576637 0.55399063 0.51523255]\n",
      " [0.51812192 0.52437305 0.49042856]\n",
      " ...\n",
      " [0.5171917  0.56048631 0.49979778]\n",
      " [0.51135008 0.5171699  0.49674573]\n",
      " [0.52124519 0.49500264 0.49222561]] \n",
      "\n",
      "Thetas:  [[ 0.0191183  -0.00303351 -0.01434214  0.0011153 ]\n",
      " [-0.01271168  0.00066393  0.00540602 -0.01318897]] \n",
      "\n",
      "\t\t\t\tShape (2, 4) \n",
      "\n",
      "Current Activation: \n",
      " [[0.50240114 0.49602153]\n",
      " [0.50252296 0.49596263]\n",
      " [0.5026432  0.4959998 ]\n",
      " ...\n",
      " [0.50251704 0.49601756]\n",
      " [0.50267593 0.49596811]\n",
      " [0.50274664 0.4959547 ]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "A:  [[0.50240114 0.49602153]\n",
      " [0.50252296 0.49596263]\n",
      " [0.5026432  0.4959998 ]\n",
      " ...\n",
      " [0.50251704 0.49601756]\n",
      " [0.50267593 0.49596811]\n",
      " [0.50274664 0.4959547 ]] \n",
      "\n",
      "Thetas:  [[ 0.00845426  0.00131092  0.00349084]\n",
      " [ 0.00404068  0.00512434  0.01120362]\n",
      " [ 0.00860454  0.0048691  -0.0076434 ]\n",
      " [ 0.00286331 -0.00557834 -0.01448752]\n",
      " [-0.00041377 -0.0091281   0.01135141]\n",
      " [-0.00037709  0.00045263  0.00775206]\n",
      " [-0.00418124 -0.00256595  0.01164671]\n",
      " [-0.01383679 -0.00866955 -0.00133694]\n",
      " [ 0.00784789  0.00249792 -0.01157517]\n",
      " [-0.00919774  0.0169446   0.00316269]] \n",
      "\n",
      "\t\t\t\tShape (10, 3) \n",
      "\n",
      "Current Activation: \n",
      " [[0.50271107 0.50304306 0.50181487 ... 0.49528626 0.50084033 0.500221  ]\n",
      " [0.50271106 0.50304305 0.50181513 ... 0.49528601 0.50084058 0.50022147]\n",
      " [0.50271113 0.50304331 0.5018152  ... 0.49528574 0.50084054 0.50022201]\n",
      " ...\n",
      " [0.50271111 0.5030432  0.50181502 ... 0.49528601 0.50084041 0.50022149]\n",
      " [0.50271112 0.50304326 0.5018153  ... 0.49528568 0.50084066 0.50022212]\n",
      " [0.50271113 0.50304332 0.50181541 ... 0.49528553 0.50084074 0.50022241]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "Activations: \n",
      " [array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0.52728472, 0.59161295, 0.51185904],\n",
      "       [0.54576637, 0.55399063, 0.51523255],\n",
      "       [0.51812192, 0.52437305, 0.49042856],\n",
      "       ...,\n",
      "       [0.5171917 , 0.56048631, 0.49979778],\n",
      "       [0.51135008, 0.5171699 , 0.49674573],\n",
      "       [0.52124519, 0.49500264, 0.49222561]]), array([[0.50240114, 0.49602153],\n",
      "       [0.50252296, 0.49596263],\n",
      "       [0.5026432 , 0.4959998 ],\n",
      "       ...,\n",
      "       [0.50251704, 0.49601756],\n",
      "       [0.50267593, 0.49596811],\n",
      "       [0.50274664, 0.4959547 ]]), array([[0.50271107, 0.50304306, 0.50181487, ..., 0.49528626, 0.50084033,\n",
      "        0.500221  ],\n",
      "       [0.50271106, 0.50304305, 0.50181513, ..., 0.49528601, 0.50084058,\n",
      "        0.50022147],\n",
      "       [0.50271113, 0.50304331, 0.5018152 , ..., 0.49528574, 0.50084054,\n",
      "        0.50022201],\n",
      "       ...,\n",
      "       [0.50271111, 0.5030432 , 0.50181502, ..., 0.49528601, 0.50084041,\n",
      "        0.50022149],\n",
      "       [0.50271112, 0.50304326, 0.5018153 , ..., 0.49528568, 0.50084066,\n",
      "        0.50022212],\n",
      "       [0.50271113, 0.50304332, 0.50181541, ..., 0.49528553, 0.50084074,\n",
      "        0.50022241]])]\n",
      "\n",
      "Last Layer Error: \n",
      " \n",
      "[None, None, None,               0         1         2         3         4         5         6  \\\n",
      "0      0.502711  0.503043  0.501815  0.498219  0.500158  0.500924  0.500077   \n",
      "1     -0.497289  0.503043  0.501815  0.498219  0.500157  0.500924  0.500076   \n",
      "2     -0.497289  0.503043  0.501815  0.498218  0.500157  0.500924  0.500076   \n",
      "3      0.502711  0.503043  0.501815 -0.501781  0.500157  0.500924  0.500076   \n",
      "4     -0.497289  0.503043  0.501815  0.498218  0.500158  0.500924  0.500077   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "59995  0.502711  0.503043  0.501815  0.498218  0.500157 -0.499076  0.500077   \n",
      "59996  0.502711 -0.496957  0.501815  0.498218  0.500157  0.500924  0.500077   \n",
      "59997  0.502711  0.503043  0.501815 -0.501781  0.500157  0.500924  0.500077   \n",
      "59998 -0.497289  0.503043  0.501815  0.498218  0.500157  0.500924  0.500076   \n",
      "59999  0.502711  0.503043  0.501815  0.498218  0.500157 -0.499076  0.500076   \n",
      "\n",
      "              7         8         9  \n",
      "0      0.495286  0.500840 -0.499779  \n",
      "1      0.495286  0.500841  0.500221  \n",
      "2      0.495286  0.500841  0.500222  \n",
      "3      0.495286  0.500841  0.500222  \n",
      "4      0.495286  0.500840  0.500221  \n",
      "...         ...       ...       ...  \n",
      "59995  0.495286  0.500840  0.500222  \n",
      "59996  0.495286  0.500840  0.500222  \n",
      "59997  0.495286  0.500840  0.500221  \n",
      "59998  0.495286  0.500841  0.500222  \n",
      "59999  0.495286  0.500841  0.500222  \n",
      "\n",
      "[60000 rows x 10 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(back_prop(thetas, X_train_scaled, Y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
