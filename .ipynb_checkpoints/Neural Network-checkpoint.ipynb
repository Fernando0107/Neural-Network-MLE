{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "## Juan Fernando Gonzalez\n",
    "20170085"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import scipy.optimize as op\n",
    "from utils import mnist_reader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns the sigmoid activation of a given input.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    s = (1/(1 + np.e**(-x)))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward2(thetas, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the all activations of all neurons of a layer.\n",
    "    \n",
    "    activations:\n",
    "        array: Keep the activation given the sigmoid function. (vector)\n",
    "    Bias:\n",
    "        Bias added to our activations. (vector)\n",
    "    thetas: \n",
    "        Weight of each connection. (Matrix)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    activations = []\n",
    "    activations.append(X)                     # Input Neurons (layer 0)\n",
    "    bias = np.ones(len(X)).reshape(len(X), 1) # Bias added (vector of ones) \n",
    "    \n",
    "    for i in range (len(thetas)):\n",
    "        \n",
    "        print('A:',activations[i])            # Actual activation \n",
    "        print('Thetas',thetas[i])             # Actual Theta\n",
    "        print('Shape',thetas[i].shape)        # Shape of the actual theta\n",
    "        \n",
    "                                   # np.dot(activations(with bias), current_theta(weigh))\n",
    "        activations.append(sigmoid(np.dot(np.hstack((bias, activations[i])), thetas[i].T))) \n",
    "        \n",
    "        print('Current Activation: \\n' , activations[i])\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(thetas, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the all activations of all neurons of a layer.\n",
    "    \n",
    "    activations:\n",
    "        array: Keep the activation given the sigmoid function. (vector)\n",
    "    Bias:\n",
    "        Bias added to our activations. (vector)\n",
    "    thetas: \n",
    "        Weight of each connection. (Matrix)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    activations = []\n",
    "    activations.append(X)                     # Input Neurons (layer 0)\n",
    "    bias = np.ones(len(X)).reshape(len(X), 1) # Bias added (vector of ones) \n",
    "    \n",
    "    for i in range (len(thetas)):\n",
    "        \n",
    "        print('A:',activations[i])            # Actual activation \n",
    "        print('Thetas',thetas[i])             # Actual Theta\n",
    "        print('Shape',thetas[i].shape)        # Shape of the actual theta\n",
    "        \n",
    "                                   # np.dot(activations, current_theta(weigh) + bias)\n",
    "        activations.append(sigmoid(np.dot(activations[i], thetas[i]) + bias))\n",
    "        \n",
    "        print('Current Activation: \\n' , activations[i + 1], '\\n')\n",
    "        print('\\t --------------------------------------------------------------------- \\n')\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(act, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = (-1/m) * (np.dot(np.log(A), Y.T) + np.dot(log(1 - A), 1 - Y.T)) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(act, X, Y):\n",
    "    \n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thetas calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brain_thatas(layer_dims):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    thetas = []\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        thetas.append(np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01)\n",
    "\n",
    "        \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data normalized shape (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "norm = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_scaled = norm.fit_transform(X_train)\n",
    "df_normalized_train = pd.DataFrame(X_train_scaled)\n",
    "\n",
    "print('Training data normalized shape' ,X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data normalized shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "X_test_scaled = norm.fit_transform(X_test)\n",
    "df_normalized_test = pd.DataFrame(X_test_scaled)\n",
    "\n",
    "print('Training data normalized shape' , X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1],\n",
    "             [1, 1, 1],\n",
    "             [1, 0, 1],\n",
    "             [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([0, 1, 1, 0]).T\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenL_1 = np.zeros((3, 1))\n",
    "\n",
    "hiddenL_2 = np.zeros((2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Layers =  input_l + hidden_layers + output_l\n",
    "\n",
    "total_l = []\n",
    "total_l.append(len(X))\n",
    "total_l.append(len(hiddenL_1))\n",
    "#total_l.append(len(hiddenL_2))\n",
    "total_l.append(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:  [4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions: ', total_l)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights (Thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (0): \n",
      " [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539  0.01744812 -0.00761207]\n",
      " [ 0.00319039 -0.0024937   0.01462108 -0.02060141]]\n",
      "\t\tShape:  (3, 4) \n",
      "\n",
      "Theta (1): \n",
      " [[-0.00322417 -0.00384054  0.01133769]\n",
      " [-0.01099891 -0.00172428 -0.00877858]\n",
      " [ 0.00042214  0.00582815 -0.01100619]\n",
      " [ 0.01144724  0.00901591  0.00502494]]\n",
      "\t\tShape:  (4, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thetas = brain_thatas(total_l)\n",
    "\n",
    "for i in range (len(thetas)):\n",
    "    print('Theta (' + str(i) +'): \\n', thetas[i])\n",
    "    print('\\t\\tShape: ',  thetas[i].shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward - Get activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[0 0 1]\n",
      " [1 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]]\n",
      "Thetas [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539  0.01744812 -0.00761207]\n",
      " [ 0.00319039 -0.0024937   0.01462108 -0.02060141]]\n",
      "Shape (3, 4)\n",
      "Current Activation: \n",
      " [[0.73168538 0.730568   0.73392353 0.72698887]\n",
      " [0.73654503 0.72479515 0.73629261 0.72333335]\n",
      " [0.7348623  0.72936214 0.73289084 0.7248541 ]\n",
      " [0.73338096 0.72601373 0.73731686 0.72547545]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "A: [[0.73168538 0.730568   0.73392353 0.72698887]\n",
      " [0.73654503 0.72479515 0.73629261 0.72333335]\n",
      " [0.7348623  0.72936214 0.73289084 0.7248541 ]\n",
      " [0.73338096 0.72601373 0.73731686 0.72547545]]\n",
      "Thetas [[-0.00322417 -0.00384054  0.01133769]\n",
      " [-0.01099891 -0.00172428 -0.00877858]\n",
      " [ 0.00042214  0.00582815 -0.01100619]\n",
      " [ 0.01144724  0.00901591  0.00502494]]\n",
      "Shape (4, 3)\n",
      "Current Activation: \n",
      " [[0.73071187 0.73238601 0.73055843]\n",
      " [0.73071324 0.73238055 0.7305705 ]\n",
      " [0.73070757 0.73237907 0.73056773]\n",
      " [0.73071752 0.73238748 0.73056123]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "Activations: \n",
      " [array([[0, 0, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 0, 1],\n",
      "       [0, 1, 1]]), array([[0.73168538, 0.730568  , 0.73392353, 0.72698887],\n",
      "       [0.73654503, 0.72479515, 0.73629261, 0.72333335],\n",
      "       [0.7348623 , 0.72936214, 0.73289084, 0.7248541 ],\n",
      "       [0.73338096, 0.72601373, 0.73731686, 0.72547545]]), array([[0.73071187, 0.73238601, 0.73055843],\n",
      "       [0.73071324, 0.73238055, 0.7305705 ],\n",
      "       [0.73070757, 0.73237907, 0.73056773],\n",
      "       [0.73071752, 0.73238748, 0.73056123]])]\n"
     ]
    }
   ],
   "source": [
    "print('Activations: \\n' ,feed_forward(thetas, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
