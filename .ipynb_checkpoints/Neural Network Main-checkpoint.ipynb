{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "## Juan Fernando Gonzalez\n",
    "20170085"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import scipy.optimize as op\n",
    "from utils import mnist_reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns the sigmoid activation of a given input.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    s = (1/(1 + np.e**(-x)))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(thetas, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the all activations of all neurons of a layer.\n",
    "    \n",
    "    activations:\n",
    "        array: Keep the activation given the sigmoid function. (vector)\n",
    "    Bias:\n",
    "        Bias added to our activations. (vector)\n",
    "    thetas: \n",
    "        Weight of each connection. (Matrix)\n",
    "    \n",
    "    \"\"\"\n",
    "    print('\\n\\t -------------------------- Feed Forward ------------------------------ \\n')\n",
    "    activations = []\n",
    "    activations.append(X)                     # Input Neurons (layer 0)\n",
    "    bias = np.ones(len(X)).reshape(len(X), 1) # Bias added (vector of ones) \n",
    "    \n",
    "    for i in range (len(thetas)):\n",
    "        \n",
    "        print('A: ',activations[i], '\\n')            # Actual activation \n",
    "        print('Thetas: ',thetas[i], '\\n')             # Actual Theta\n",
    "        print('\\t\\t\\t\\tShape',thetas[i].shape, '\\n')        # Shape of the actual theta\n",
    "        \n",
    "                                   # np.dot(activations(with bias), current_theta(weigh))\n",
    "        activations.append(sigmoid(np.dot(np.hstack((bias, activations[i])), thetas[i].T))) \n",
    "        \n",
    "        print('Current Activation: \\n' , activations[i + 1], '\\n')\n",
    "        print('\\t --------------------------------------------------------------------- \\n')\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(thetas, X, Y):\n",
    "    \n",
    "    m = len(X)\n",
    "    \n",
    "    act = feed_forward(thetas, X)\n",
    "    \n",
    "    cost = (-1/m) * (np.dot(np.log(act), Y.T) + np.dot(log(1 - act), 1 - Y.T)) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(thetas, X, Y):\n",
    "    \n",
    "    m , neu = X.shape\n",
    "    \n",
    "    delta = [None] * len(thetas)\n",
    "    \n",
    "    activations = feed_forward(thetas, X)\n",
    "    \n",
    "    print('Activations: \\n' , activations)\n",
    "    \n",
    "    print('\\n\\t -----------------------------End Feed Forward ----------------------------- \\n')\n",
    "    # Last layer error\n",
    "    delta_ = activations[-1] - Y\n",
    "    delta.append(delta_)\n",
    "    \n",
    "    print(\"\\nLast Layer Error: \\n \")\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in reversed(range(1, len(thetas) -1)):\n",
    "        \n",
    "        layers_error = np.matmul(np.dot(thetas[i].T, delta_), np.dot(activations[i], (1 - activations[i])))\n",
    "        \n",
    "        delta.append(layers_error)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return delta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thetas calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brain_thatas(layer_dims):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a Theta-Matrix array.\n",
    "    \n",
    "    L:\n",
    "        Get layers lenght.\n",
    "    l:\n",
    "        Iterator; get the dim of the current theta.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    thetas = []\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        thetas.append(np.random.randn(layer_dims[l], layer_dims[l - 1] + 1) * 0.01)\n",
    "\n",
    "        \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data normalized shape (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "norm = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_scaled = norm.fit_transform(X_train)\n",
    "\n",
    "print('Training data normalized shape' , X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data normalized shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "X_test_scaled = norm.fit_transform(X_test)\n",
    "df_normalized_test = pd.DataFrame(X_test_scaled)\n",
    "\n",
    "print('Training data normalized shape' , X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Neurons on the Output Layer:  10\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(y_train)\n",
    "Y_test = pd.get_dummies(y_test)\n",
    "\n",
    "\n",
    "print('No. of Neurons on the Output Layer: ', Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenL_1 = np.zeros((150, 1))\n",
    "\n",
    "hiddenL_2 = np.zeros((150, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Layers =  input layer + hidden layers + output layer\n",
    "\n",
    "total_l = []\n",
    "\n",
    "total_l.append(X_train_scaled.shape[1])\n",
    "total_l.append(len(hiddenL_1))\n",
    "total_l.append(len(hiddenL_2))\n",
    "total_l.append(Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:  [784, 150, 150, 10]\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions: ', total_l)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights (Thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t --------------- Thetas ---------------\n",
      "\n",
      "Theta (0): \n",
      " [[ 0.01624345 -0.00611756 -0.00528172 ... -0.00359224  0.00505382\n",
      "   0.01217941]\n",
      " [-0.01940681 -0.00806178  0.00049062 ...  0.00626906  0.00299825\n",
      "  -0.01856641]\n",
      " [-0.02151043  0.00136301  0.00683356 ... -0.00045206  0.00342846\n",
      "  -0.02164928]\n",
      " ...\n",
      " [-0.00831628  0.01750997  0.00971161 ...  0.00543153 -0.00816227\n",
      "   0.00644091]\n",
      " [ 0.01256017 -0.00017165 -0.00611912 ...  0.01157191 -0.00359375\n",
      "  -0.00283559]\n",
      " [-0.0020656  -0.00856748 -0.01457998 ...  0.00611592  0.00476244\n",
      "   0.0074112 ]]\n",
      "\t\tShape:  (150, 785) \n",
      "\n",
      "Theta (1): \n",
      " [[ 0.01694211 -0.01664437 -0.01549058 ... -0.0059683   0.00911969\n",
      "  -0.00369576]\n",
      " [-0.00619904 -0.01928083 -0.00998663 ... -0.01008846 -0.00978995\n",
      "   0.00442066]\n",
      " [-0.00169847  0.00980928 -0.00499157 ...  0.01306511 -0.00494019\n",
      "  -0.00338944]\n",
      " ...\n",
      " [-0.00121764 -0.01824912  0.00664456 ... -0.01292536  0.00031973\n",
      "  -0.00118959]\n",
      " [ 0.01108233  0.01216136 -0.00253871 ...  0.00914982  0.01650309\n",
      "   0.01794047]\n",
      " [ 0.00240736 -0.01428304 -0.02445331 ... -0.0139121   0.02323056\n",
      "   0.00050788]]\n",
      "\t\tShape:  (150, 151) \n",
      "\n",
      "Theta (2): \n",
      " [[ 0.00489391  0.00824539 -0.00318875 ... -0.00622529  0.00401546\n",
      "   0.00154463]\n",
      " [ 0.00714269  0.00361243 -0.01939652 ...  0.00468653 -0.00861673\n",
      "  -0.01428706]\n",
      " [ 0.00011901  0.00590527  0.0066501  ...  0.01062731 -0.00563667\n",
      "  -0.00728139]\n",
      " ...\n",
      " [-0.00579782 -0.0122016  -0.00493307 ...  0.01353007 -0.00609041\n",
      "  -0.01456636]\n",
      " [ 0.00068042 -0.00036972 -0.00690192 ...  0.02124827 -0.00932378\n",
      "   0.00108581]\n",
      " [-0.00218961  0.00360077  0.00598402 ... -0.00655996 -0.00057944\n",
      "  -0.01207751]]\n",
      "\t\tShape:  (10, 151) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "thetas = brain_thatas(total_l)\n",
    "\n",
    "print('\\t --------------- Thetas ---------------\\n')\n",
    "for i in range (len(thetas)):\n",
    "    print('Theta (' + str(i) +'): \\n', thetas[i])\n",
    "    print('\\t\\tShape: ',  thetas[i].shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t -------------------------- Feed Forward ------------------------------ \n",
      "\n",
      "A:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "Thetas:  [[ 0.01624345 -0.00611756 -0.00528172 ... -0.00359224  0.00505382\n",
      "   0.01217941]\n",
      " [-0.01940681 -0.00806178  0.00049062 ...  0.00626906  0.00299825\n",
      "  -0.01856641]\n",
      " [-0.02151043  0.00136301  0.00683356 ... -0.00045206  0.00342846\n",
      "  -0.02164928]\n",
      " ...\n",
      " [-0.00831628  0.01750997  0.00971161 ...  0.00543153 -0.00816227\n",
      "   0.00644091]\n",
      " [ 0.01256017 -0.00017165 -0.00611912 ...  0.01157191 -0.00359375\n",
      "  -0.00283559]\n",
      " [-0.0020656  -0.00856748 -0.01457998 ...  0.00611592  0.00476244\n",
      "   0.0074112 ]] \n",
      "\n",
      "\t\t\t\tShape (150, 785) \n",
      "\n",
      "Current Activation: \n",
      " [[0.52728472 0.59161295 0.51185904 ... 0.46210074 0.51825007 0.52425974]\n",
      " [0.54576637 0.55399063 0.51523255 ... 0.49553723 0.5328826  0.53991898]\n",
      " [0.51812192 0.52437305 0.49042856 ... 0.48079879 0.51974177 0.51792087]\n",
      " ...\n",
      " [0.5171917  0.56048631 0.49979778 ... 0.49478045 0.54893277 0.53068367]\n",
      " [0.51135008 0.5171699  0.49674573 ... 0.48588987 0.52091011 0.5080791 ]\n",
      " [0.52124519 0.49500264 0.49222561 ... 0.48762128 0.48978605 0.52035181]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "A:  [[0.52728472 0.59161295 0.51185904 ... 0.46210074 0.51825007 0.52425974]\n",
      " [0.54576637 0.55399063 0.51523255 ... 0.49553723 0.5328826  0.53991898]\n",
      " [0.51812192 0.52437305 0.49042856 ... 0.48079879 0.51974177 0.51792087]\n",
      " ...\n",
      " [0.5171917  0.56048631 0.49979778 ... 0.49478045 0.54893277 0.53068367]\n",
      " [0.51135008 0.5171699  0.49674573 ... 0.48588987 0.52091011 0.5080791 ]\n",
      " [0.52124519 0.49500264 0.49222561 ... 0.48762128 0.48978605 0.52035181]] \n",
      "\n",
      "Thetas:  [[ 0.01694211 -0.01664437 -0.01549058 ... -0.0059683   0.00911969\n",
      "  -0.00369576]\n",
      " [-0.00619904 -0.01928083 -0.00998663 ... -0.01008846 -0.00978995\n",
      "   0.00442066]\n",
      " [-0.00169847  0.00980928 -0.00499157 ...  0.01306511 -0.00494019\n",
      "  -0.00338944]\n",
      " ...\n",
      " [-0.00121764 -0.01824912  0.00664456 ... -0.01292536  0.00031973\n",
      "  -0.00118959]\n",
      " [ 0.01108233  0.01216136 -0.00253871 ...  0.00914982  0.01650309\n",
      "   0.01794047]\n",
      " [ 0.00240736 -0.01428304 -0.02445331 ... -0.0139121   0.02323056\n",
      "   0.00050788]] \n",
      "\n",
      "\t\t\t\tShape (150, 151) \n",
      "\n",
      "Current Activation: \n",
      " [[0.48245012 0.51449607 0.50841652 ... 0.48349436 0.50956188 0.4609232 ]\n",
      " [0.48172795 0.51362643 0.50630936 ... 0.484896   0.50932143 0.46139404]\n",
      " [0.48237557 0.51446383 0.50794071 ... 0.48425851 0.51037643 0.46237026]\n",
      " ...\n",
      " [0.48159442 0.51434784 0.50737464 ... 0.48436716 0.50954844 0.46329314]\n",
      " [0.48278408 0.51354072 0.50744861 ... 0.48424396 0.5095319  0.46141819]\n",
      " [0.48264552 0.51435853 0.50884258 ... 0.48360173 0.51007815 0.46118475]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "A:  [[0.48245012 0.51449607 0.50841652 ... 0.48349436 0.50956188 0.4609232 ]\n",
      " [0.48172795 0.51362643 0.50630936 ... 0.484896   0.50932143 0.46139404]\n",
      " [0.48237557 0.51446383 0.50794071 ... 0.48425851 0.51037643 0.46237026]\n",
      " ...\n",
      " [0.48159442 0.51434784 0.50737464 ... 0.48436716 0.50954844 0.46329314]\n",
      " [0.48278408 0.51354072 0.50744861 ... 0.48424396 0.5095319  0.46141819]\n",
      " [0.48264552 0.51435853 0.50884258 ... 0.48360173 0.51007815 0.46118475]] \n",
      "\n",
      "Thetas:  [[ 0.00489391  0.00824539 -0.00318875 ... -0.00622529  0.00401546\n",
      "   0.00154463]\n",
      " [ 0.00714269  0.00361243 -0.01939652 ...  0.00468653 -0.00861673\n",
      "  -0.01428706]\n",
      " [ 0.00011901  0.00590527  0.0066501  ...  0.01062731 -0.00563667\n",
      "  -0.00728139]\n",
      " ...\n",
      " [-0.00579782 -0.0122016  -0.00493307 ...  0.01353007 -0.00609041\n",
      "  -0.01456636]\n",
      " [ 0.00068042 -0.00036972 -0.00690192 ...  0.02124827 -0.00932378\n",
      "   0.00108581]\n",
      " [-0.00218961  0.00360077  0.00598402 ... -0.00655996 -0.00057944\n",
      "  -0.01207751]] \n",
      "\n",
      "\t\t\t\tShape (10, 151) \n",
      "\n",
      "Current Activation: \n",
      " [[0.50204176 0.52036264 0.5010259  ... 0.49737769 0.49193362 0.48841166]\n",
      " [0.5020472  0.52032434 0.5010739  ... 0.49737174 0.49190711 0.48846406]\n",
      " [0.50212563 0.52032233 0.50106078 ... 0.49736062 0.49190573 0.48841716]\n",
      " ...\n",
      " [0.50210897 0.52032199 0.5010733  ... 0.49738214 0.49192745 0.48839837]\n",
      " [0.50208594 0.52033111 0.50107282 ... 0.49734902 0.4919174  0.48841944]\n",
      " [0.50208669 0.52033263 0.50107552 ... 0.49734863 0.49191023 0.48840498]] \n",
      "\n",
      "\t --------------------------------------------------------------------- \n",
      "\n",
      "Activations: \n",
      " [array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0.52728472, 0.59161295, 0.51185904, ..., 0.46210074, 0.51825007,\n",
      "        0.52425974],\n",
      "       [0.54576637, 0.55399063, 0.51523255, ..., 0.49553723, 0.5328826 ,\n",
      "        0.53991898],\n",
      "       [0.51812192, 0.52437305, 0.49042856, ..., 0.48079879, 0.51974177,\n",
      "        0.51792087],\n",
      "       ...,\n",
      "       [0.5171917 , 0.56048631, 0.49979778, ..., 0.49478045, 0.54893277,\n",
      "        0.53068367],\n",
      "       [0.51135008, 0.5171699 , 0.49674573, ..., 0.48588987, 0.52091011,\n",
      "        0.5080791 ],\n",
      "       [0.52124519, 0.49500264, 0.49222561, ..., 0.48762128, 0.48978605,\n",
      "        0.52035181]]), array([[0.48245012, 0.51449607, 0.50841652, ..., 0.48349436, 0.50956188,\n",
      "        0.4609232 ],\n",
      "       [0.48172795, 0.51362643, 0.50630936, ..., 0.484896  , 0.50932143,\n",
      "        0.46139404],\n",
      "       [0.48237557, 0.51446383, 0.50794071, ..., 0.48425851, 0.51037643,\n",
      "        0.46237026],\n",
      "       ...,\n",
      "       [0.48159442, 0.51434784, 0.50737464, ..., 0.48436716, 0.50954844,\n",
      "        0.46329314],\n",
      "       [0.48278408, 0.51354072, 0.50744861, ..., 0.48424396, 0.5095319 ,\n",
      "        0.46141819],\n",
      "       [0.48264552, 0.51435853, 0.50884258, ..., 0.48360173, 0.51007815,\n",
      "        0.46118475]]), array([[0.50204176, 0.52036264, 0.5010259 , ..., 0.49737769, 0.49193362,\n",
      "        0.48841166],\n",
      "       [0.5020472 , 0.52032434, 0.5010739 , ..., 0.49737174, 0.49190711,\n",
      "        0.48846406],\n",
      "       [0.50212563, 0.52032233, 0.50106078, ..., 0.49736062, 0.49190573,\n",
      "        0.48841716],\n",
      "       ...,\n",
      "       [0.50210897, 0.52032199, 0.5010733 , ..., 0.49738214, 0.49192745,\n",
      "        0.48839837],\n",
      "       [0.50208594, 0.52033111, 0.50107282, ..., 0.49734902, 0.4919174 ,\n",
      "        0.48841944],\n",
      "       [0.50208669, 0.52033263, 0.50107552, ..., 0.49734863, 0.49191023,\n",
      "        0.48840498]])]\n",
      "\n",
      "\t -----------------------------End Feed Forward ----------------------------- \n",
      "\n",
      "\n",
      "Last Layer Error: \n",
      " \n",
      "[None, None, None,               0         1         2         3         4         5         6  \\\n",
      "0      0.502042  0.520363  0.501026  0.493673  0.487827  0.474764  0.494335   \n",
      "1     -0.497953  0.520324  0.501074  0.493661  0.487811  0.474791  0.494291   \n",
      "2     -0.497874  0.520322  0.501061  0.493696  0.487827  0.474775  0.494320   \n",
      "3      0.502105  0.520322  0.501067 -0.506318  0.487822  0.474771  0.494315   \n",
      "4     -0.497881  0.520337  0.501007  0.493704  0.487812  0.474785  0.494301   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "59995  0.502094  0.520355  0.501046  0.493696  0.487823 -0.525244  0.494345   \n",
      "59996  0.502131 -0.479651  0.501048  0.493685  0.487835  0.474765  0.494337   \n",
      "59997  0.502109  0.520322  0.501073 -0.506326  0.487815  0.474786  0.494322   \n",
      "59998 -0.497914  0.520331  0.501073  0.493683  0.487829  0.474768  0.494324   \n",
      "59999  0.502087  0.520333  0.501076  0.493704  0.487818 -0.525235  0.494339   \n",
      "\n",
      "              7         8         9  \n",
      "0      0.497378  0.491934 -0.511588  \n",
      "1      0.497372  0.491907  0.488464  \n",
      "2      0.497361  0.491906  0.488417  \n",
      "3      0.497342  0.491908  0.488432  \n",
      "4      0.497378  0.491925  0.488425  \n",
      "...         ...       ...       ...  \n",
      "59995  0.497315  0.491933  0.488405  \n",
      "59996  0.497383  0.491925  0.488407  \n",
      "59997  0.497382  0.491927  0.488398  \n",
      "59998  0.497349  0.491917  0.488419  \n",
      "59999  0.497349  0.491910  0.488405  \n",
      "\n",
      "[60000 rows x 10 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(back_prop(thetas, X_train_scaled, Y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
